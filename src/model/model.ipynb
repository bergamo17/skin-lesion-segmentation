{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nclass Reshape(nn.Module):\n    def __init__(self, *shape):\n        super(Reshape, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.reshape(x.size(0), *self.shape)\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=100):\n        super(Generator, self).__init__()\n        self.latent_dim = latent_dim\n        self._initialize_weights()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, 512 * 7 * 7),\n            nn.BatchNorm1d(512 * 7 * 7),\n            nn.LeakyReLU(0.01, inplace=False),\n            Reshape(512, 7, 7),\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.01, inplace=False),\n            \n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.01, inplace=False),\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.01, inplace=False),\n            nn.ConvTranspose2d(64, output_shape[2], kernel_size=4, stride=4, padding=0),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.ConvTranspose2d, nn.Linear)):\n                init.normal_(m.weight, mean=0.0, std=0.02)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                init.normal_(m.weight, mean=1.0, std=0.02)\n                nn.init.constant_(m.bias, 0)\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_classes=2, negative_slope=0.01):\n        super().__init__()\n        vgg = models.vgg16(pretrained=True)\n        feats = list(vgg.features.children())\n        self.pool3 = nn.Sequential(*feats[:17])\n        self.pool4 = nn.Sequential(*feats[17:24])\n        self.pool5 = nn.Sequential(*feats[24:])\n        \n        self.conv6 = nn.Conv2d(512,4096,7,padding=3)\n        self.act6 = nn.LeakyReLU(negative_slope, inplace=False)\n        self.drop6 = nn.Dropout2d()\n        \n        self.conv7 = nn.Conv2d(4096,4096,1,padding=0)\n        self.act7 = nn.LeakyReLU(negative_slope, inplace=False)\n        self.drop7 = nn.Dropout2d()\n        \n        self.seg_head = nn.Conv2d(4096,num_classes,1)\n        self.score_pool4 = nn.Conv2d(512,num_classes,1)\n        self.score_pool3 = nn.Conv2d(256,num_classes,1)\n        self.up2 = nn.ConvTranspose2d(num_classes,num_classes,kernel_size=4,stride=2,padding=1,bias=False)\n        self.up4 = nn.ConvTranspose2d(num_classes,num_classes,kernel_size=4,stride=2,padding=1,bias=False)\n        self.up8 = nn.ConvTranspose2d(num_classes,num_classes,kernel_size=16,stride=8,padding=4,bias=False)\n\n        self.disc_head = nn.Conv2d(4096, 1, 1)\n\n     def forward(self,x):\n        p3 = self.pool3(x)\n        p4 = self.pool4(p3)\n        p5 = self.pool5(p4)\n        \n        h = self.drop6(self.act6(self.conv6(p5)))\n        h = self.drop7(self.act7(self.conv7(h)))\n        \n        s = self.seg_head(h)\n        \n        up2 = self.up2(s)\n        s4 = self.score_pool4(p4)\n        up2 = up2[:,:,:s4.shape[2],:s4.shape[3]]\n        fuse4 = up2 + s4\n        \n        up4 = self.up4(fuse4)\n        s3 = self.score_pool3(p3)\n        up4 = up4[:,:,:s3.shape[2],:s3.shape[3]]\n        fuse3 = up4 + s3\n        \n        seg_out = self.up8(fuse3)\n        seg_out = seg_out[:, :, :x.shape[2], :x.shape[3]]\n\n        \n        disc_out = self.disc_head(h)\n\n        return seg_out, disc_out\n\nclass SGAN(nn.Module):\n    def __init__(self, generator, discriminator):\n        super(SGAN, self).__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n\n    def forward(self, x):\n        return self.discriminator(x)  \n    \n    def generate_fake(self, x):\n        return self.generator(x)\n\nbce_loss = nn.BCEWithLogitsLoss()\n\ndef discriminator_loss(disc_out_real=None, disc_out_fake=None, seg_out_labeled=None, labels_labeled=None, gamma=2.0):\n    loss_real = bce_loss(disc_out_real, torch.ones_like(disc_out_real)) if disc_out_real is not None else 0.0\n    loss_fake = bce_loss(disc_out_fake, torch.zeros_like(disc_out_fake)) if disc_out_fake is not None else 0.0\n    ce_loss = F.cross_entropy(seg_out_labeled, labels_labeled) if seg_out_labeled is not None and labels_labeled is not None else 0.0\n\n    return loss_real + loss_fake + gamma * ce_loss\n\ndef generator_loss(disc_out_fake):\n    return bce_loss(disc_out_fake, torch.ones_like(disc_out_fake))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}